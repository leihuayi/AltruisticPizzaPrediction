{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b60069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4b21c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('pizza_data.json', orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07688a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34039224",
   "metadata": {},
   "source": [
    "# Balance in users which received pizza or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_received_pizza'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_received_pizza'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940ee4c",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e610e85",
   "metadata": {},
   "source": [
    "### Reputation of the requester\n",
    "Maybe if a member is more active they have more chances of getting a pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_number_of_posts_on_raop_at_request'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_username'].value_counts().loc[lambda x : x>1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe5985",
   "metadata": {},
   "source": [
    "This means each requester has made only one request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_user_flair'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad5b8b",
   "metadata": {},
   "source": [
    "  `requester_user_flair`: Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (given after received, N=83).\n",
    "  \n",
    "  -> these numbers confirm that these badges are attributed after user received the pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304bfdc",
   "metadata": {},
   "source": [
    "## Relevant information\n",
    "\n",
    "Which columns can actually help in predicting ?\n",
    "\n",
    "**Things which do not help**\n",
    "* Unique infos : requester_username, request_id\n",
    "* Infos after the gift : giver_username_if_known, requester_user_flair\n",
    "* Things probably not relevant or related to raop : post_was_edited, requester_account_age_in_days, requester_days, requester_number_of_comments, requester_number_of_posts, requester_upvotes_plus_downvotes, listr of subreddits names\n",
    "* Things which are obviously correlated to other column: requester_text_edit_aware, unix_timestamp_of_request_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a445ff",
   "metadata": {},
   "source": [
    "## Find correlated variables\n",
    "Using Pearson correlation coefficient and p-value, on the rema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4494a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"number_of_downvotes_of_request_at_retrieval\",\n",
    "          \"number_of_upvotes_of_request_at_retrieval\",\n",
    "           \"requester_number_of_comments_in_raop_at_request\",\n",
    "           \"requester_number_of_comments_in_raop_at_retrieval\",\n",
    "           \"requester_number_of_posts_on_raop_at_request\",\n",
    "           \"requester_number_of_posts_on_raop_at_retrieval\",\n",
    "           \"requester_number_of_subreddits_at_request\",\n",
    "           \"requester_upvotes_minus_downvotes_at_request\",\n",
    "           \"requester_upvotes_minus_downvotes_at_retrieval\"\n",
    "          ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd031fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(data.corr(), annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    df = df.dropna()._get_numeric_data()\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)\n",
    "    return pvalues\n",
    "\n",
    "calculate_pvalues(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457e087",
   "metadata": {},
   "source": [
    "### Correlation results\n",
    "\n",
    "We use the threshold recommended by Evans (1996) : **correlation coeff >= 0.6** to decide that 2 variables are strongly correlated. We consider that the correlation test of 2 variables is valid if the **p-value is < 0.05**.\n",
    "According to these criteria, the following variables are correlated:\n",
    "* number_of_downvotes_of_request_at_retrieval and number_of_upvotes_of_request_at_retrieval\n",
    "* requester_number_of_comments_in_raop_at_request and requester_number_of_comments_in_raop_at_retrieval\n",
    "* requester_number_of_posts_on_raop_at_request and requester_number_of_posts_on_raop_at_retrieval\n",
    "* requester_upvotes_minus_downvotes_at_request and requester_upvotes_minus_downvotes_at_retrieval (strong)\n",
    "\n",
    "however the number of downvotes and upvotes are obviously not correlated so we keep both of them and merge them into one variable : number_of_upvotes_minus_downvotes_of_request\n",
    "\n",
    "We observe in the reddit channel that donators react quite fast to the comments (usually the same day) while there can be several weeks between request and retrieval. Therefore for a situation closer to what the donator had **we chose to keep the variables \"at_request\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382b0c7",
   "metadata": {},
   "source": [
    "# Text data\n",
    "\n",
    "Inspiration from https://towardsdatascience.com/text-classification-in-python-dd95d264c802"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9536399",
   "metadata": {},
   "source": [
    "### Difference between columns text and text_edit_aware\n",
    "\n",
    "According to https://cs.stanford.edu/~althoff/raop-dataset/,\n",
    "`We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\".` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftxt = df[['request_text','request_text_edit_aware']].copy()\n",
    "\n",
    "dftxt['same_text'] = np.where( dftxt['request_text'] == dftxt['request_text_edit_aware'] , True, False)\n",
    "dftxt['same_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61098c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftxt[dftxt['same_text']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftxt[dftxt['request_text_edit_aware']=='*']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6beaa8",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "I manually checked a few of the texts where request_text and request_text_edit_aware are not the same, a lot of times the EDIT from request_text is not about thanking a donator, it is more about adding some new contextual information which might have convinced the people to give them a pizza.\n",
    "Therefore **we prefer to use request_text** and discard column request_text_edit_aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb492b0",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1783c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86dcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"[Request] College student, pay check delayed for a week, all out of food pantry food, and haven't eaten today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ac2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuation = list(\",.?!(){}[]-_\\\"'\\\\;:+*<>@#ยง^$%&|/\") + ['\\n', '\\r', '\\t', '...', '..']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"request\")\n",
    "stop_words.add(\"edit\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tag_dict = {\"J\": wn.ADJ,\n",
    "            \"N\": wn.NOUN,\n",
    "            \"V\": wn.VERB,\n",
    "            \"R\": wn.ADV}\n",
    "\n",
    "def extract_wnpostag_from_postag(tag):\n",
    "    #take the first letter of the tag\n",
    "    #the second parameter is an \"optional\" in case of missing key in the dictionary \n",
    "    return tag_dict.get(tag[0].upper(), None)\n",
    "\n",
    "def lemmatize_tupla_word_postag(tupla):\n",
    "    \"\"\"\n",
    "    giving a tupla of the form (wordString, posTagString) like ('guitar', 'NN'), return the lemmatized word\n",
    "    \"\"\"\n",
    "    tag = extract_wnpostag_from_postag(tupla[1])    \n",
    "    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]\n",
    "\n",
    "def correspondance_miswrite(word):\n",
    "    if word == \"im\":\n",
    "        return \"i'm\"\n",
    "    elif word == \"ive\":\n",
    "        return \"i've\"\n",
    "\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    original_words = word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(original_words) #returns a list of tuples: (word, tagString) like ('And', 'CC')\n",
    "    lemmatized_words = [ lemmatize_tupla_word_postag(ow) for ow in tagged_words ]\n",
    "    cleaned_words = [ \n",
    "        w for w in lemmatized_words if (w not in punctuation) and (w not in stop_words)\n",
    "    ]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93908978",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece6ac2",
   "metadata": {},
   "source": [
    "## Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_title'] = df['request_title'].str.len()\n",
    "df['len_text'] = df['request_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sn.histplot(df['len_title']).set_title('Requests title length distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e03747",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_250 = df[df['len_title'] > 250]\n",
    "print('Num requests with very long title: ', len(title_250))\n",
    "print('\\n Example of a request:')\n",
    "print('------ Title ----------')\n",
    "print(title_250['request_title'].iloc[0])\n",
    "print('------ Text ----------')\n",
    "print(title_250['request_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a68252",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sn.boxplot(data=df, x='requester_received_pizza', y='len_title');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sn.histplot(df['len_text']).set_title('Requests text length distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "df_95 = df[df['len_text'] < df['len_text'].quantile(0.95)]\n",
    "sn.histplot(df_95['len_text']).set_title('Requests text length distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sn.boxplot(data=df_95, x='requester_received_pizza', y='len_text');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09593cc",
   "metadata": {},
   "source": [
    "## Text data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40639446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col(row, col):\n",
    "    return clean_text(row[col])\n",
    "\n",
    "df['cleaned_title'] = df.apply(lambda x: clean_col(x, 'request_title'), axis =1)\n",
    "df['cleaned_text'] = df.apply(lambda x: clean_col(x, 'request_text'), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['cleaned_title', 'cleaned_text', 'requester_received_pizza']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef27224",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "## Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_title'], \n",
    "                                                    np.array(df['requester_received_pizza']).astype(int), \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653af776",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = vectorizer.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print('Train features:', features_train.shape)\n",
    "features_test = vectorizer.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print('Test features:', features_test.shape)\n",
    "print('Identified words:')\n",
    "features_names = vectorizer.get_feature_names_out()\n",
    "print(features_names[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    features_chi2 = chi2(features_train, labels_train == i)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(features_names)[indices]\n",
    "    print(\"# Requester received Pizza {}:\".format(bool(i)))\n",
    "    print(\"Most correlated words:\\n. {}\".format('\\n. '.join(feature_names[-10:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ed5f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# X_train\n",
    "with open('pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('pickles/vectorizer.pickle', 'wb') as output:\n",
    "    pickle.dump(vectorizer, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fc19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
