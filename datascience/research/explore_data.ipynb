{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b60069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4b21c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('pizza_data.json', orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07688a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_retrieval</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_retrieval</th>\n",
       "      <th>requester_user_flair</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.111100</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, Eve, IAmA, MontereyBay, RandomKind...</td>\n",
       "      <td>34</td>\n",
       "      <td>4258</td>\n",
       "      <td>116</td>\n",
       "      <td>11168</td>\n",
       "      <td>None</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>jacquibatman7</td>\n",
       "      <td>1319650094</td>\n",
       "      <td>1319646494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>4</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>6.518438</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "      <td>None</td>\n",
       "      <td>4on_the_floor</td>\n",
       "      <td>1322855434</td>\n",
       "      <td>1322855434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>5</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>162.063252</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[GayBrosWeightLoss, RandomActsOfCookies, Rando...</td>\n",
       "      <td>1121</td>\n",
       "      <td>1225</td>\n",
       "      <td>1733</td>\n",
       "      <td>1887</td>\n",
       "      <td>None</td>\n",
       "      <td>Futuredogwalker</td>\n",
       "      <td>1373657691</td>\n",
       "      <td>1373654091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "2                     N/A                                            0   \n",
       "3                     N/A                                            0   \n",
       "4                     N/A                                            6   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "2                                          3                0   t3_lpu5j   \n",
       "3                                          1                1   t3_mxvj3   \n",
       "4                                          6                0  t3_1i6486   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        4   \n",
       "4                                        5   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "4  hey guys:\\n I love this sub. I think it's grea...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "4  hey guys:\\n I love this sub. I think it's grea...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "2  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  ...  requester_received_pizza  \\\n",
       "0                                  0.000000  ...                     False   \n",
       "1                                501.111100  ...                     False   \n",
       "2                                  0.000000  ...                     False   \n",
       "3                                  6.518438  ...                     False   \n",
       "4                                162.063252  ...                     False   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0                                                 []   \n",
       "1  [AskReddit, Eve, IAmA, MontereyBay, RandomKind...   \n",
       "2                                                 []   \n",
       "3       [AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]   \n",
       "4  [GayBrosWeightLoss, RandomActsOfCookies, Rando...   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                             0   \n",
       "1                                            34   \n",
       "2                                             0   \n",
       "3                                            54   \n",
       "4                                          1121   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
       "0                                               1   \n",
       "1                                            4258   \n",
       "2                                               3   \n",
       "3                                              59   \n",
       "4                                            1225   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request  \\\n",
       "0                                            0   \n",
       "1                                          116   \n",
       "2                                            0   \n",
       "3                                           76   \n",
       "4                                         1733   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\n",
       "0                                              1                  None   \n",
       "1                                          11168                  None   \n",
       "2                                              3                  None   \n",
       "3                                             81                  None   \n",
       "4                                           1887                  None   \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "2       jacquibatman7                 1319650094   \n",
       "3       4on_the_floor                 1322855434   \n",
       "4     Futuredogwalker                 1373657691   \n",
       "\n",
       "   unix_timestamp_of_request_utc  \n",
       "0                     1317849007  \n",
       "1                     1332648824  \n",
       "2                     1319646494  \n",
       "3                     1322855434  \n",
       "4                     1373654091  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34039224",
   "metadata": {},
   "source": [
    "# Balance in users which received pizza or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ff73b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3046\n",
       "True      994\n",
       "Name: requester_received_pizza, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['requester_received_pizza'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3cec58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQzElEQVR4nO3df6zddX3H8eeLFtAJE5ALwf5YGetiSjaRNYBhWZxEKLismG0KmdoZkvpHyTS6P4rZhlFZcJuykSBJDZ2wMAnzR2hmM+wYCzEZSmEEKIxxRZB2QCsgoky0+N4f51M54r29t+3tOXg/z0dycr7f9/fz/Z73N2lf93s/53vOTVUhSerDIeNuQJI0Ooa+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHFo67gb059thja9myZeNuQ5J+odx5553fqaqJqba9okN/2bJlbN26ddxtSNIvlCSPTrfN6R1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR2b8cFaSVwG3AYe38V+oqkuTnAjcALwOuBN4T1X9KMnhwHXAbwFPAe+qqkfasS4BLgJeBP60qm6e+1MavWXrvzLuFuaVRy5/+7hbkOat2VzpvwC8tareCJwCrEpyBvBJ4Iqq+jXgGQZhTnt+ptWvaONIsgK4ADgZWAV8JsmCOTwXSdIMZgz9Gvh+Wz20PQp4K/CFVr8WOL8tr27rtO1nJUmr31BVL1TVt4BJ4LS5OAlJ0uzMak4/yYIkdwM7gS3AN4HvVtXuNmQ7sKgtLwIeA2jbn2UwBfTT+hT7SJJGYFahX1UvVtUpwGIGV+dvOFgNJVmbZGuSrbt27TpYLyNJXdqnu3eq6rvArcCbgaOS7HkjeDGwoy3vAJYAtO2vZfCG7k/rU+wz/BobqmplVa2cmJjym0ElSftpxtBPMpHkqLb8auBtwAMMwv8P27A1wE1teVNbp23/96qqVr8gyeHtzp/lwDfm6DwkSbMwm+/TPwG4tt1pcwhwY1X9S5L7gRuSfAL4L+CaNv4a4B+TTAJPM7hjh6raluRG4H5gN7Cuql6c29ORJO3NjKFfVfcAb5qi/jBT3H1TVT8E/miaY10GXLbvbUqS5oKfyJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjswY+kmWJLk1yf1JtiX5QKt/NMmOJHe3x3lD+1ySZDLJg0nOGaqvarXJJOsPzilJkqazcBZjdgMfrqq7khwJ3JlkS9t2RVX97fDgJCuAC4CTgdcD/5bk19vmq4C3AduBO5Jsqqr75+JEJEkzmzH0q+px4PG2/FySB4BFe9llNXBDVb0AfCvJJHBa2zZZVQ8DJLmhjTX0JWlE9mlOP8ky4E3A11vp4iT3JNmY5OhWWwQ8NrTb9labri5JGpFZh36SI4AvAh+squ8BVwMnAacw+E3gU3PRUJK1SbYm2bpr1665OKQkqZlV6Cc5lEHgX19VXwKoqier6sWq+gnwWV6awtkBLBnafXGrTVf/GVW1oapWVtXKiYmJfT0fSdJezObunQDXAA9U1aeH6icMDXsHcF9b3gRckOTwJCcCy4FvAHcAy5OcmOQwBm/2bpqb05AkzcZs7t45E3gPcG+Su1vtI8CFSU4BCngEeD9AVW1LciODN2h3A+uq6kWAJBcDNwMLgI1VtW3OzkSSNKPZ3L3zNSBTbNq8l30uAy6bor55b/tJkg4uP5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2ZMfSTLElya5L7k2xL8oFWPybJliQPteejWz1JrkwymeSeJKcOHWtNG/9QkjUH77QkSVOZzZX+buDDVbUCOANYl2QFsB64paqWA7e0dYBzgeXtsRa4GgY/JIBLgdOB04BL9/ygkCSNxoyhX1WPV9Vdbfk54AFgEbAauLYNuxY4vy2vBq6rgduBo5KcAJwDbKmqp6vqGWALsGouT0aStHf7NKefZBnwJuDrwPFV9Xjb9ARwfFteBDw2tNv2VpuuLkkakVmHfpIjgC8CH6yq7w1vq6oCai4aSrI2ydYkW3ft2jUXh5QkNbMK/SSHMgj866vqS638ZJu2oT3vbPUdwJKh3Re32nT1n1FVG6pqZVWtnJiY2JdzkSTNYDZ37wS4Bnigqj49tGkTsOcOnDXATUP197a7eM4Anm3TQDcDZyc5ur2Be3arSZJGZOEsxpwJvAe4N8ndrfYR4HLgxiQXAY8C72zbNgPnAZPA88D7AKrq6SQfB+5o4z5WVU/PxUlIkmZnxtCvqq8BmWbzWVOML2DdNMfaCGzclwYlSXPHT+RKUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkdmDP0kG5PsTHLfUO2jSXYkubs9zhvadkmSySQPJjlnqL6q1SaTrJ/7U5EkzWQ2V/qfA1ZNUb+iqk5pj80ASVYAFwAnt30+k2RBkgXAVcC5wArgwjZWkjRCC2caUFW3JVk2y+OtBm6oqheAbyWZBE5r2yar6mGAJDe0sffve8uSpP11IHP6Fye5p03/HN1qi4DHhsZsb7Xp6pKkEdrf0L8aOAk4BXgc+NRcNZRkbZKtSbbu2rVrrg4rSWI/Q7+qnqyqF6vqJ8BneWkKZwewZGjo4labrj7VsTdU1cqqWjkxMbE/7UmSprFfoZ/khKHVdwB77uzZBFyQ5PAkJwLLgW8AdwDLk5yY5DAGb/Zu2v+2JUn7Y8Y3cpN8HngLcGyS7cClwFuSnAIU8AjwfoCq2pbkRgZv0O4G1lXVi+04FwM3AwuAjVW1ba5PRpK0d7O5e+fCKcrX7GX8ZcBlU9Q3A5v3qTtJ0pzyE7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JEZQz/JxiQ7k9w3VDsmyZYkD7Xno1s9Sa5MMpnkniSnDu2zpo1/KMmag3M6kqS9mc2V/ueAVS+rrQduqarlwC1tHeBcYHl7rAWuhsEPCeBS4HTgNODSPT8oJEmjM2PoV9VtwNMvK68Grm3L1wLnD9Wvq4HbgaOSnACcA2ypqqer6hlgCz//g0SSdJDt75z+8VX1eFt+Aji+LS8CHhsat73VpqtLkkZo4YEeoKoqSc1FMwBJ1jKYGmLp0qVzdVipW8vWf2XcLcwbj1z+9nG3cMD290r/yTZtQ3ve2eo7gCVD4xa32nT1n1NVG6pqZVWtnJiY2M/2JElT2d/Q3wTsuQNnDXDTUP297S6eM4Bn2zTQzcDZSY5ub+Ce3WqSpBGacXonyeeBtwDHJtnO4C6cy4Ebk1wEPAq8sw3fDJwHTALPA+8DqKqnk3wcuKON+1hVvfzNYUnSQTZj6FfVhdNsOmuKsQWsm+Y4G4GN+9SdJGlO+YlcSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIAYV+kkeS3Jvk7iRbW+2YJFuSPNSej271JLkyyWSSe5KcOhcnIEmavbm40v/dqjqlqla29fXALVW1HLilrQOcCyxvj7XA1XPw2pKkfXAwpndWA9e25WuB84fq19XA7cBRSU44CK8vSZrGgYZ+AV9NcmeSta12fFU93pafAI5vy4uAx4b23d5qkqQRWXiA+/92Ve1IchywJcl/D2+sqkpS+3LA9sNjLcDSpUsPsD1J0rADutKvqh3teSfwZeA04Mk90zbteWcbvgNYMrT74lZ7+TE3VNXKqlo5MTFxIO1Jkl5mv0M/yWuSHLlnGTgbuA/YBKxpw9YAN7XlTcB72108ZwDPDk0DSZJG4ECmd44Hvpxkz3H+qar+NckdwI1JLgIeBd7Zxm8GzgMmgeeB9x3Aa0uS9sN+h35VPQy8cYr6U8BZU9QLWLe/rydJOnB+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjLy0E+yKsmDSSaTrB/160tSz0Ya+kkWAFcB5wIrgAuTrBhlD5LUs1Ff6Z8GTFbVw1X1I+AGYPWIe5Ckbo069BcBjw2tb281SdIILBx3Ay+XZC2wtq1+P8mD4+xnnjkW+M64m5hJPjnuDjQmr/h/n79A/zZ/ZboNow79HcCSofXFrfZTVbUB2DDKpnqRZGtVrRx3H9JU/Pc5GqOe3rkDWJ7kxCSHARcAm0bcgyR1a6RX+lW1O8nFwM3AAmBjVW0bZQ+S1LORz+lX1WZg86hfV4DTZnpl89/nCKSqxt2DJGlE/BoGSeqIoS9JHTH0JY1cBt6d5C/b+tIkp427rx4Y+vNckl9K8hdJPtvWlyf5vXH3pe59BngzcGFbf47B93LpIDP0579/AF5g8B8MBh+G+8T42pEAOL2q1gE/BKiqZ4DDxttSHwz9+e+kqvpr4McAVfU8kPG2JPHj9q27BZBkAvjJeFvqg6E///0oyat56T/XSQyu/KVxuhL4MnBcksuArwF/Nd6W+uB9+vNckrcBf87g7xd8FTgT+JOq+o9x9iUleQNwFoPfPG+pqgfG3FIXDP0OJHkdcAaD/1y3V9Ur+psMNf8lWTpVvaq+PepeemPoz3NJzgTurqofJHk3cCrw91X16JhbU8eS3MtgyjHAq4ATgQer6uSxNtYB5/Tnv6uB55O8EfgQ8E3guvG2pN5V1W9U1W+25+UM/qref467rx4Y+vPf7hr8OrcauKqqrgKOHHNP0s+oqruA08fdRw9ecX85S3PuuSSXAO8GfifJIcChY+5JnUvyoaHVQxhMO/7vmNrpilf689+7GNyieVFVPcHgr5X9zXhbkjhy6HE48BUGv43qIPONXEkj1T6U9cmq+rNx99Ijp3fmqSTP0T6Q9fJNQFXVL4+4JYkkC9tf0Dtz3L30yit9SSOT5K6qOjXJ1cAi4J+BH+zZXlVfGltznfBKvxNJjmNwPzTgh2A0dq8CngLeykv36xdg6B9khv48l+T3gU8Brwd2Ar8CPAD4IRiNw3Htzp37eCns93DaYQS8e2f++ziDr2D4n6o6kcF3ndw+3pbUsQXAEe1x5NDynocOMq/0578fV9VTSQ5JckhV3Zrk78bdlLr1eFV9bNxN9MzQn/++m+QI4Dbg+iQ7GXrjTBox/5bDmHn3zjyVZGlVfTvJa4D/YzCV98fAa4Hrq+qpsTaoLiU5pqqeHncfPTP056k9t8a15S9W1R+MuydJ4+cbufPX8K/Rvzq2LiS9ohj681dNsyypY07vzFNJXmTwhm2AVwPP79mEX8MgdcvQl6SOOL0jSR0x9CWpI4a+JHXE0Jekjhj6ktSR/wfP6Wihp2KY/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['requester_received_pizza'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940ee4c",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e610e85",
   "metadata": {},
   "source": [
    "### Reputation of the requester\n",
    "Maybe if a member is more active they have more chances of getting a pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_number_of_posts_on_raop_at_request'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_username'].value_counts().loc[lambda x : x>1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe5985",
   "metadata": {},
   "source": [
    "This means each requester has made only one request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requester_user_flair'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad5b8b",
   "metadata": {},
   "source": [
    "  `requester_user_flair`: Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (given after received, N=83).\n",
    "  \n",
    "  -> these numbers confirm that these badges are attributed after user received the pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304bfdc",
   "metadata": {},
   "source": [
    "## Relevant information\n",
    "\n",
    "Which columns can actually help in predicting ?\n",
    "\n",
    "**Things which do not help**\n",
    "* Unique infos : requester_username, request_id\n",
    "* Infos after the gift : giver_username_if_known, requester_user_flair\n",
    "* Things probably not relevant or related to raop : post_was_edited, requester_account_age_in_days, requester_days, requester_number_of_comments, requester_number_of_posts, requester_upvotes_plus_downvotes, listr of subreddits names\n",
    "* Things which are obviously correlated to other column: requester_text_edit_aware, unix_timestamp_of_request_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a445ff",
   "metadata": {},
   "source": [
    "## Find correlated variables\n",
    "Using Pearson correlation coefficient and p-value, on the rema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4494a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"number_of_downvotes_of_request_at_retrieval\",\n",
    "          \"number_of_upvotes_of_request_at_retrieval\",\n",
    "           \"requester_number_of_comments_in_raop_at_request\",\n",
    "           \"requester_number_of_comments_in_raop_at_retrieval\",\n",
    "           \"requester_number_of_posts_on_raop_at_request\",\n",
    "           \"requester_number_of_posts_on_raop_at_retrieval\",\n",
    "           \"requester_number_of_subreddits_at_request\",\n",
    "           \"requester_upvotes_minus_downvotes_at_request\",\n",
    "           \"requester_upvotes_minus_downvotes_at_retrieval\"\n",
    "          ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd031fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(data.corr(), annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    df = df.dropna()._get_numeric_data()\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)\n",
    "    return pvalues\n",
    "\n",
    "calculate_pvalues(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457e087",
   "metadata": {},
   "source": [
    "### Correlation results\n",
    "\n",
    "We use the threshold recommended by Evans (1996) : **correlation coeff >= 0.6** to decide that 2 variables are strongly correlated. We consider that the correlation test of 2 variables is valid if the **p-value is < 0.05**.\n",
    "According to these criteria, the following variables are correlated:\n",
    "* number_of_downvotes_of_request_at_retrieval and number_of_upvotes_of_request_at_retrieval\n",
    "* requester_number_of_comments_in_raop_at_request and requester_number_of_comments_in_raop_at_retrieval\n",
    "* requester_number_of_posts_on_raop_at_request and requester_number_of_posts_on_raop_at_retrieval\n",
    "* requester_upvotes_minus_downvotes_at_request and requester_upvotes_minus_downvotes_at_retrieval (strong)\n",
    "\n",
    "however the number of downvotes and upvotes are obviously not correlated so we keep both of them and merge them into one variable : number_of_upvotes_minus_downvotes_of_request\n",
    "\n",
    "We observe in the reddit channel that donators react quite fast to the comments (usually the same day) while there can be several weeks between request and retrieval. Therefore for a situation closer to what the donator had **we chose to keep the variables \"at_request\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382b0c7",
   "metadata": {},
   "source": [
    "# Text data\n",
    "\n",
    "Inspiration from https://towardsdatascience.com/text-classification-in-python-dd95d264c802"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9536399",
   "metadata": {},
   "source": [
    "### Difference between columns text and text_edit_aware\n",
    "\n",
    "According to https://cs.stanford.edu/~althoff/raop-dataset/,\n",
    "`We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\".` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftxt = df[['request_text','request_text_edit_aware']].copy()\n",
    "\n",
    "dftxt['same_text'] = np.where( dftxt['request_text'] == dftxt['request_text_edit_aware'] , True, False)\n",
    "dftxt['same_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61098c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftxt[dftxt['same_text']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftxt[dftxt['request_text_edit_aware']=='*']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6beaa8",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "I manually checked a few of the texts where request_text and request_text_edit_aware are not the same, a lot of times the EDIT from request_text is not about thanking a donator, it is more about adding some new contextual information which might have convinced the people to give them a pizza.\n",
    "Therefore **we prefer to use request_text** and discard column request_text_edit_aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb492b0",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1783c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86dcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"[Request] College student, pay check delayed for a week, all out of food pantry food, and haven't eaten today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50ac2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuation = list(\",.?!(){}[]-_\\\"'\\\\;:+*<>@#§^$%&|/\") + ['\\n', '\\r', '\\t', '...', '..']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"request\")\n",
    "stop_words.add(\"edit\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tag_dict = {\"J\": wn.ADJ,\n",
    "            \"N\": wn.NOUN,\n",
    "            \"V\": wn.VERB,\n",
    "            \"R\": wn.ADV}\n",
    "\n",
    "def extract_wnpostag_from_postag(tag):\n",
    "    #take the first letter of the tag\n",
    "    #the second parameter is an \"optional\" in case of missing key in the dictionary \n",
    "    return tag_dict.get(tag[0].upper(), None)\n",
    "\n",
    "def lemmatize_tupla_word_postag(tupla):\n",
    "    \"\"\"\n",
    "    giving a tupla of the form (wordString, posTagString) like ('guitar', 'NN'), return the lemmatized word\n",
    "    \"\"\"\n",
    "    tag = extract_wnpostag_from_postag(tupla[1])    \n",
    "    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]\n",
    "\n",
    "def correspondance_miswrite(word):\n",
    "    if word == \"im\":\n",
    "        return \"i'm\"\n",
    "    elif word == \"ive\":\n",
    "        return \"i've\"\n",
    "\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    original_words = word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(original_words) #returns a list of tuples: (word, tagString) like ('And', 'CC')\n",
    "    lemmatized_words = [ lemmatize_tupla_word_postag(ow) for ow in tagged_words ]\n",
    "    cleaned_words = [ \n",
    "        w for w in lemmatized_words if (w not in punctuation) and (w not in stop_words)\n",
    "    ]\n",
    "    cleaned_words = [ correspondance_miswrite(w) for w in cleaned_words ]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93908978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"college student pay check delay week food pantry food n't eat today\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece6ac2",
   "metadata": {},
   "source": [
    "## Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_title'] = df['request_title'].str.len()\n",
    "df['len_text'] = df['request_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sn.histplot(df['len_title']).set_title('Requests title length distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e03747",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_250 = df[df['len_title'] > 250]\n",
    "print('Num requests with very long title: ', len(title_250))\n",
    "print('\\n Example of a request:')\n",
    "print('------ Title ----------')\n",
    "print(title_250['request_title'].iloc[0])\n",
    "print('------ Text ----------')\n",
    "print(title_250['request_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a68252",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sn.boxplot(data=df, x='requester_received_pizza', y='len_title');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sn.histplot(df['len_text']).set_title('Requests text length distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "df_95 = df[df['len_text'] < df['len_text'].quantile(0.95)]\n",
    "sn.histplot(df_95['len_text']).set_title('Requests text length distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sn.boxplot(data=df_95, x='requester_received_pizza', y='len_text');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09593cc",
   "metadata": {},
   "source": [
    "## Text data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40639446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col(row, col):\n",
    "    return clean_text(row[col])\n",
    "\n",
    "df['cleaned_title'] = df.apply(lambda x: clean_col(x, 'request_title'), axis =1)\n",
    "df['cleaned_text'] = df.apply(lambda x: clean_col(x, 'request_text'), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b618ce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>colorado spring help us please</td>\n",
       "      <td>hi need food 4 child military family really hi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california cash could use dinner</td>\n",
       "      <td>spend last money gas today im break next thursday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hungry couple dundee scotland would love pizza</td>\n",
       "      <td>girlfriend decide would good idea get perth bu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canada ontario get home school need pizza</td>\n",
       "      <td>'s cold i'n hungry completely honest 'm broke ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>old friend come visit would love fee dinner in...</td>\n",
       "      <td>hey guy love sub think 's great except sob sto...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>anyone help recent college grad start new job/...</td>\n",
       "      <td>anyone kind enough help start new job monday d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>usa papa john give away one free pizza 15 purc...</td>\n",
       "      <td>someone could hook 15 gift card would happily ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>mi usa day would love pizza lunch</td>\n",
       "      <td>today soo 'll stick house day clean homework '...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>nashua nh mother one hungry 2 year old</td>\n",
       "      <td>'ve never anything like willing try proud moth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>usa wa unexpected bill could n't go grocery sh...</td>\n",
       "      <td>like title say pay unexpected bill go pay come...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4040 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned_title  \\\n",
       "0                        colorado spring help us please   \n",
       "1                      california cash could use dinner   \n",
       "2        hungry couple dundee scotland would love pizza   \n",
       "3             canada ontario get home school need pizza   \n",
       "4     old friend come visit would love fee dinner in...   \n",
       "...                                                 ...   \n",
       "4035  anyone help recent college grad start new job/...   \n",
       "4036  usa papa john give away one free pizza 15 purc...   \n",
       "4037                  mi usa day would love pizza lunch   \n",
       "4038             nashua nh mother one hungry 2 year old   \n",
       "4039  usa wa unexpected bill could n't go grocery sh...   \n",
       "\n",
       "                                           cleaned_text  \\\n",
       "0     hi need food 4 child military family really hi...   \n",
       "1     spend last money gas today im break next thursday   \n",
       "2     girlfriend decide would good idea get perth bu...   \n",
       "3     's cold i'n hungry completely honest 'm broke ...   \n",
       "4     hey guy love sub think 's great except sob sto...   \n",
       "...                                                 ...   \n",
       "4035  anyone kind enough help start new job monday d...   \n",
       "4036  someone could hook 15 gift card would happily ...   \n",
       "4037  today soo 'll stick house day clean homework '...   \n",
       "4038  've never anything like willing try proud moth...   \n",
       "4039  like title say pay unexpected bill go pay come...   \n",
       "\n",
       "      requester_received_pizza  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                        False  \n",
       "4                        False  \n",
       "...                        ...  \n",
       "4035                     False  \n",
       "4036                      True  \n",
       "4037                     False  \n",
       "4038                     False  \n",
       "4039                     False  \n",
       "\n",
       "[4040 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_title', 'cleaned_text', 'requester_received_pizza']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef27224",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "## Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2dc1f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eed7440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df['cleaned_text']), \n",
    "                                                    np.array(df['requester_received_pizza']).astype(int), \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81b1b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"get pay end month final week always struggle could ration pizza pizza 'm willing return favor take funny picture involve cat pizza\"\n",
      " 'parent visit uncle vermont service provide money order dinner night university id steal since dont money cant get new one pizza would help get weekend get new one please help'\n",
      " \"check history 'll see last three week credit card skim car window bust stitch put back without insurance etc etc today get electric bill 320 dollar 17 year old thermostat decide 'd cool run ac heat time work part time non-profit n't pay much everything 's happen tap couple month pizza probably last three day leftover goodness 've make noodle rice food bank past week 'm hungry hate scammer intend pay forward problem use verification code/pming address/sending printscreen adorable graph bill quadrupling/etc thank friends\"\n",
      " \"'m 21 year old boston college student bank account overdraw monday 've live ramen noodle old brownie make last week pizza would really help get weekend pay forward back prefer monday get check thanks advance whoever help\"\n",
      " \"well 's first time subreddit wish give unfortunately though able pay rent n't money friday buy grocery anything similar would really help could get pizza munch study tonight thanks great upcoming thankgiving everyone\"\n",
      " \"get new smoker week ago decide want make beercan chicken first time never use smoker smoke food normal grill anyway walmart mesquite hickory wood chip turn terrible could n't get smoker proper temp smoke `` flavor '' one bad thing ever taste needless say wont ever could help send pizza n't go bed empty stomach would really appreciate\"\n",
      " \"n't pizza\"\n",
      " \"'ve probably eat one meal past 3 day broke n't get pay tuesday house devoid food pizza would much appreciate right take picture verification 'd like\"\n",
      " \"'s hungry day office 'm self employ 'd willing web design work show immense gratitude likely complete whatever kind project need\"\n",
      " 'stick small vilage uk call south killingholme near grimsby grim name suggest car buss suck miss pizza unsure deliver would hope someone prove wrong']\n",
      "[\"girlfriend three year move first apartment together little one bedroom small college town 're student employ move prove costly n't food fridge car break earlier today move stuff apartment random act pizza would cure hunger would also make day ten time well fridge proof http //imgur.com/qq446 thanks reading\"\n",
      " \"post last night perhaps bit late evening zero fund food speak 've hungry day car accident week ago life pretty crummy late know long shot anyone feel like help thanks pandyfackerlling\"\n",
      " \"lose job four week 'not meeting sale target hardly training bos fire know full well mortgage kitten look car stop light show peugeot 207 fuck french shit money bank 'm wit end please help throwaway account\"\n",
      " \"morning breakfast consume essentially leave actual food cheese bacon hamburger bun salad currently 'm train new job find hard focus without bring lunch full day nice hot gooey pizza dinner lunch tomorrow 'd set pizza hut nice deal go buy pizza trade body/hygiene/cosmetic product future store message want know store really awesome one thank\"\n",
      " \"hi 've run food n't money buy food get pay tomorrow please kind redditor order pizza dominos.co.uk .if pay credit/debit card walk dominos 30 minute away collect £5.99 cheap pizza use paypal need order expensive pizza least £8.99 deliver £3.20 paypal account send kind enough buy one please pm detail forever grateful 'm scotland uk name pizza tandoori hot tandoori chicken onion mushroom green pepper jalapeno pepper 198kcal large slice personal 4 slice £5.99\"\n",
      " ''\n",
      " \"possible 'd like papa john couple current promo buy pizza get one free tomorrow thanks advance\"\n",
      " \"last month 've eat rice noodle chicken broth least five time week dinner pizza would welcome change would much appreciate 's four us n't need much\"\n",
      " \"really huge story get pull last week get ticket 'm shortttt cash 'll friday night buy next guy pizza\"\n",
      " \"pm 'd like help celebrate pass final\"]\n",
      "[0 0 1 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:10])\n",
    "print(X_test[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "653af776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: (3232, 1000)\n",
      "Test features: (808, 1000)\n",
      "Identified words:\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_features=1000)\n",
    "                        \n",
    "features_train = vectorizer.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print('Train features:', features_train.shape)\n",
    "features_test = vectorizer.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print('Test features:', features_test.shape)\n",
    "print('Identified words:')\n",
    "features_names = vectorizer.get_feature_names_out()\n",
    "print(features_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "582cf002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': False,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': 1000,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3dc91415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.2846478  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.29940933 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.15691147 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.24406966 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.24915712 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.29848886 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.11195991 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.36950286 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.18287071 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13508844 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.25470607 0.         0.\n",
      " 0.26836446 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.22571417 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.30034385\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.19422436 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.1524658  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.23278471\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(features_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a2b1fa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top true words:\n",
      ". willing\n",
      ". end\n",
      ". favor\n",
      ". picture\n",
      ". always\n",
      ". pizza\n",
      ". final\n",
      ". cat\n",
      ". struggle\n",
      ". involve\n",
      "Top false words:\n",
      ". work\n",
      ". money\n",
      ". food\n",
      ". go\n",
      ". really\n",
      ". help\n",
      ". pay\n",
      ". get\n",
      ". would\n",
      ". pizza\n"
     ]
    }
   ],
   "source": [
    "X_1 = features_train[labels_train].sum(axis=0)\n",
    "X_0 = features_train[np.logical_not(labels_train)].sum(axis=0)\n",
    "top_true = np.argsort(X_1)[-10:]\n",
    "print(\"Top true words:\\n. {}\".format('\\n. '.join(np.array(features_names)[top_true])))\n",
    "top_false = np.argsort(X_0)[-10:]\n",
    "print(\"Top false words:\\n. {}\".format('\\n. '.join(np.array(features_names)[top_false])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b3a0f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Requester received Pizza False:\n",
      "Most correlated words:\n",
      ". http\n",
      ". com\n",
      ". fulfil\n",
      ". landlord\n",
      ". check\n",
      ". sunday\n",
      ". surprise\n",
      ". rice\n",
      ". jpg\n",
      ". imgur\n",
      "\n",
      "# Requester received Pizza True:\n",
      "Most correlated words:\n",
      ". http\n",
      ". com\n",
      ". fulfil\n",
      ". landlord\n",
      ". check\n",
      ". sunday\n",
      ". surprise\n",
      ". rice\n",
      ". jpg\n",
      ". imgur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    features_chi2 = chi2(features_train, labels_train == i)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(features_names)[indices]\n",
    "    print(\"# Requester received Pizza {}:\".format(bool(i)))\n",
    "    print(\"Most discriminating words words:\\n. {}\".format('\\n. '.join(feature_names[-10:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ed5f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# X_train\n",
    "with open('pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('pickles/vectorizer.pickle', 'wb') as output:\n",
    "    pickle.dump(vectorizer, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fc19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
